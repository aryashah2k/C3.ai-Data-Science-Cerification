{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75d055d1",
   "metadata": {},
   "source": [
    "Use this notebook as a template to complete the Capstone. This notebook assumes that you have successfully run the previous notebook 05.1_C3_Capstone_Setup_MDF and have at least one MLModel deployed as `LIVE` on the test population segment. It also assumes that you have a trained and tuned pipeline wihch has acheived an F1 score of at least 0.8 that you have used to create an MLModel at the end of the notebook 04_C3_Model_Development_Experimentation. Once you have this MLModel created, you can use this notebook to deploy that model as a `CHALLENGER` model. \n",
    "\n",
    "Your goal is to have a `CHALLENGER` model that achieves an F1-Score of at least 0.80 on the Test Segment. Please don't train on the test segment. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66734800",
   "metadata": {},
   "source": [
    "# Deploy Challenger Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a45e50",
   "metadata": {},
   "source": [
    "Necessary imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b81027",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T07:49:49.488538Z",
     "start_time": "2022-04-12T07:49:49.332833Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "def monitor_job_status(job, sleepFor=10):\n",
    "    \n",
    "    while (job.status().status not in [\"completed\", \"failed\", \"completed_with_errors\"]):\n",
    "        print(job.status().status)\n",
    "        time.sleep(sleepFor)\n",
    "        \n",
    "    return job.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dc0d9a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Retrieve the MLPopulation Segments we currently have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce36a03f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T07:49:50.558103Z",
     "start_time": "2022-04-12T07:49:50.486757Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(c3.MLPopulationSegment.fetch().objs.toJson())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc23ab9",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Get Test Segment id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190a52c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T09:06:26.504220Z",
     "start_time": "2022-04-12T09:06:26.425722Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Look in the table above to find the id of the MLPopulation segment for TestingBulbs.\n",
    "# Make sure to use the id that is attached to an MLProject (does not have NaN in the MLProject column).\n",
    "\n",
    "test_segment = c3.MLPopulationSegment.get('MLPopulationSegmentIdForTestingBulbs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ea8ee1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T07:52:15.790096Z",
     "start_time": "2022-04-12T07:52:15.552835Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "c3.MLPopulationSegment.refreshCalcFields(c3.RefreshCalcFieldsSpec(calcFieldsToRefresh=['primaryConfiguration'],\n",
    "                                                                  ids=[test_segment.id]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b447efbe",
   "metadata": {},
   "source": [
    "### What is the current primary deployed model on the Test Segment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62c9623",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T07:52:19.111230Z",
     "start_time": "2022-04-12T07:52:19.047403Z"
    }
   },
   "outputs": [],
   "source": [
    "primary_model_name = test_segment.evaluate(\n",
    "    c3.EvaluateSpec(filter=f'id==\"{test_segment.id}\"',\n",
    "                    projection='primaryConfiguration.predictionModels[0].model.pipeline.steps[0].name'\n",
    ")).tuples[0].cells[0].value()\n",
    "print(f\"The primary configuration has a model with name: {primary_model_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1a9126",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T07:58:59.457136Z",
     "start_time": "2022-04-12T07:58:59.454875Z"
    }
   },
   "outputs": [],
   "source": [
    "# # see all models deployed in this MLProject and their deployment status\n",
    "\n",
    "# pd.DataFrame(c3.MLProjectSubjectToModelRelation.fetch().objs.toJson())\n",
    "\n",
    "# # you can also view this table in the developer console to better view the nested/linked fields with\n",
    "# # c3Grid(MLProjectSubjectToModelRelation.fetch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8495d6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T08:00:49.177967Z",
     "start_time": "2022-04-12T08:00:49.063588Z"
    }
   },
   "outputs": [],
   "source": [
    "primary_model_dict = pd.DataFrame(c3.MLProjectSubjectToModelRelation.fetch({\n",
    "                                                                            \"filter\": \"isPrimary == true\"\n",
    "                                                                          }).objs.toJson())[\"to\"].tolist()\n",
    "ids = []\n",
    "for item in primary_model_dict:\n",
    "    ids.append(item[\"id\"])\n",
    "    \n",
    "ids = list(set(ids))\n",
    "# print(ids)\n",
    "if len(ids) == 1:\n",
    "    primary_model_id = ids[0]\n",
    "    print(f\"The primary model deployed on the test segment has the id: {primary_model_id}.\")\n",
    "elif len(ids) > 1: \n",
    "    print(\"There appears to be more than 1 LIVE model deployed in this project!\")\n",
    "    print(ids)\n",
    "else:\n",
    "    print(\"No LIVE model could be found for this project!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152b124d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Retrieve a MLModel that you wish to deploy as `CHALLENGER`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c05a13",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The id here should be the id of a previously trained and tuned MLModel from **Section 13** of your experimentation notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728a27a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T08:04:00.380055Z",
     "start_time": "2022-04-12T08:04:00.213506Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "challenger_model = c3.MLModel.get('MyTrainedAndTunedMLModelId') # MLModel id from the last cell in notebook 04"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc7fba8",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Deploy model as a `CHALLENGER` Model on the Test Segment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f6e2ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T08:04:25.495168Z",
     "start_time": "2022-04-12T08:04:24.932972Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "prediction_config = test_segment.deployModels([challenger_model.configuration], c3.MLModelLabel.CHALLENGER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86edd327",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T08:04:26.616006Z",
     "start_time": "2022-04-12T08:04:26.147836Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_segment.updateModels()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9ae4e0",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Check that your model has successfully deployed as a `CHALLENGER` Model on the Test Segment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbd8f66",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame(c3.MLProjectSubjectToModelRelation.fetch().objs.toJson())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e028dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T08:19:02.288648Z",
     "start_time": "2022-04-12T08:19:02.256482Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame(pd.DataFrame(c3.MLProjectSubjectToModelRelation.fetch().objs.toJson())[\"status\"])\n",
    "pd.DataFrame(c3.MLProjectSubjectToModelRelation.fetch({\n",
    "                                                                            \"filter\": f\"to == '{challenger_model.id}'\"\n",
    "                                                                          }).objs.toJson())[\"status\"].tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71239246",
   "metadata": {},
   "source": [
    "### ðŸŽ‰ðŸŽ‰ðŸŽ‰\n",
    "### Awesome! You've just deployed your trained, tuned model in an existing production system as a ```CHALLENGER``` model, using all of the features and hyperparameters you determined, exactly as you designed them.\n",
    "### ðŸŽ‰ðŸŽ‰ðŸŽ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7d9005",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Now let's compare the ```CHALLENGER``` model to the ```LIVE``` model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a23a5d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Run a `Prediction Job` to automatically generate and persist predictions for all active models on the test segment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b01abfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T08:25:44.052210Z",
     "start_time": "2022-04-12T08:25:43.739382Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pred_job = test_segment.predict(None, c3.MLPredictionJobOptions(batchSize=1), \n",
    "                                      c3.MLModelPredictSpec(allActiveModels=True,\n",
    "                                      pipelineSpec=c3.MLProcessSpec(disableGpu=True)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d996b69a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T08:27:05.380960Z",
     "start_time": "2022-04-12T08:25:44.745722Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# pred_job.status()\n",
    "\n",
    "monitor_job_status(pred_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5482c8cd",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Compare `LIVE` and `CHALLENGER` models:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947df458",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Retrieve and plot predictions for one bulb in the test segment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa764213",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T08:29:15.378450Z",
     "start_time": "2022-04-12T08:29:13.969926Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predictions_emr = c3.MLModel.evalMetrics(spec=c3.EvalMetricsSpec(\n",
    "    ids=[challenger_model.id], expressions=[\"MLProjectPrediction\"], \n",
    "    start=\"2016-01-01\", end=\"2021-01-01\",\n",
    "    bindings=[{\"subjectId\": \"SMBLB23\"}], # change your subject id here to another id in the test segment to see other predictions\n",
    "    interval=\"HOUR\",\n",
    "    resultKey=c3.Lambda.fromPython(\"lambda expression, bindings: expression + ('_' + bindings.get('featName', '') if expression == 'MLProjectContribution' else '')\")\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540ecaf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T08:29:16.374641Z",
     "start_time": "2022-04-12T08:29:15.380012Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "c3.EvalMetricsResult.toPandas(predictions_emr, multiIndexed=True).droplevel(0).plot(figsize=(16, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad7e15b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Run a `Scoring Job` to automatically generate and persist predictions for all active models on the test segment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7825b5bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T08:29:43.555955Z",
     "start_time": "2022-04-12T08:29:43.098017Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "score_job = test_segment.score(None, c3.MLScoreJobOptions(batchSize=10), \n",
    "                                      c3.MLModelScoreSpec(allActiveModels=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bb96ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T08:32:05.919988Z",
     "start_time": "2022-04-12T08:29:45.006484Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "while (score_job.status().status not in ['completed', 'completed with errors', 'failed']):\n",
    "    print(score_job.status().status)\n",
    "    time.sleep(10)\n",
    "    \n",
    "# score_job.status()\n",
    "\n",
    "monitor_job_status(score_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f139bb",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Retrieve Results from the Scoring Job for the `CHALLENGER` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e53c39b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T08:33:24.957046Z",
     "start_time": "2022-04-12T08:33:23.529599Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "start = challenger_model.get(\"scores.data.this\").scores[0].data[0].start\n",
    "scores_emr = c3.MLModel.evalMetrics(spec=c3.EvalMetricsSpec(\n",
    "                ids=[challenger_model.id], expressions=[\"Score\"], \n",
    "                start=start - timedelta(1), end=start + timedelta(hours=12),\n",
    "                bindings=[{'scoringMetricName': 'MLF1ScoreMetric'}],\n",
    "                interval=\"DAY\",\n",
    "                resultKey=c3.Lambda.fromPython(\"lambda expression, bindings: expression + '_' + bindings['scoringMetricName']\")\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46e4953",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T08:33:24.964429Z",
     "start_time": "2022-04-12T08:33:24.958600Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "scores_df = c3.EvalMetricsResult.toPandas(scores_emr, multiIndexed=True).droplevel(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181c2e58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T08:33:25.009007Z",
     "start_time": "2022-04-12T08:33:24.965986Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9202bcb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T08:33:26.974905Z",
     "start_time": "2022-04-12T08:33:26.813579Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c3.EvalMetricsResult.toPandas(scores_emr, multiIndexed=True).droplevel(0).plot(figsize=(12, 4), marker='x', grid=True, subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f356d05",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Call an instance of the `LIVE` model deployed on the test segment, using the id of the primary model you retrieved earlier in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c35d7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T08:35:31.272761Z",
     "start_time": "2022-04-12T08:35:31.198922Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "live_model = c3.MLModel.get(primary_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d18c8e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Retrieve Results from the Scoring Job for the `LIVE` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a893d333",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T08:36:40.382973Z",
     "start_time": "2022-04-12T08:36:38.699019Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start = live_model.get(\"scores.data.this\").scores[0].data[0].start\n",
    "scores_emr = c3.MLModel.evalMetrics(spec=c3.EvalMetricsSpec(\n",
    "                ids=[live_model.id], expressions=[\"Score\"], \n",
    "                start=start - timedelta(1), end=start + timedelta(hours=12),\n",
    "                bindings=[{'scoringMetricName': 'MLF1ScoreMetric'}],\n",
    "                interval=\"DAY\",\n",
    "                resultKey=c3.Lambda.fromPython(\"lambda expression, bindings: expression + '_' + bindings['scoringMetricName']\")\n",
    "))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec4da00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T08:36:40.390245Z",
     "start_time": "2022-04-12T08:36:40.384417Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "scores_df = c3.EvalMetricsResult.toPandas(scores_emr, multiIndexed=True).droplevel(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf25a75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T08:36:40.936593Z",
     "start_time": "2022-04-12T08:36:40.930170Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8040f2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T13:24:23.259189Z",
     "start_time": "2022-04-08T13:24:23.142661Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "c3.EvalMetricsResult.toPandas(scores_emr, multiIndexed=True).droplevel(0).plot(figsize=(12, 4), marker='x', grid=True, subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aad06c6",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## **Congratulations** \n",
    "on successfully deploying multiple models on the test segment and successfully comparing their performance! If you are satisfied with the performance of your `CHALLENGER` model, submit this model's id for your capstone submission. If you would like to go back to notebook 04 and continue to iterate and experiment to improve model performance, feel free to do so -- you can deploy as many `CHALLENGER` models as you would like!"
   ]
  }
 ],
 "metadata": {
  "has_local_update": true,
  "is_local": true,
  "is_remote": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "last_sync_time": "2022-04-12T09:35:06.937546"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
