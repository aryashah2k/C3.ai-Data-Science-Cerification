{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 C3 Platform Machine Learning Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Machine Learning Pipelines on the C3 AI Suite](#1)\n",
    "    * [0. Import packages and Helper Functions](#1.1)\n",
    "    * [1. Retrieve Data](#1.2)\n",
    "    * [2. Test-Train Split](#1.3)\n",
    "    * [3. Specify Preprocessing Pipeline](#1.4)\n",
    "    * [4. Specify Classifier](#1.5)\n",
    "    * [5. Create Hetrogeneous Pipeline](#1.6)\n",
    "    * [6. Train Pipeline](#1.7)\n",
    "    * [7. Score Pipeline and Analyze Results](#1.8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 0. Import the necessary packages <a class=\"anchor\" id=\"1.1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T17:29:54.969325Z",
     "start_time": "2021-10-29T17:29:54.330436Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import collections\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (10,6)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Helper methods that you defined in the previous module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T17:29:54.973195Z",
     "start_time": "2021-10-29T17:29:54.970823Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Implemented in previous challenge\n",
    "'''\n",
    "def spec_to_emr_to_df(source_type: object,\n",
    "                      spec: object,\n",
    "                      on_the_fly: bool,\n",
    "                      overrideMetrics: list\n",
    "             ):\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T17:30:01.317894Z",
     "start_time": "2021-10-29T17:30:01.315452Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Implemented in previous challenge\n",
    "'''\n",
    "def plot_metrics(df, ids, expressions):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We would like to explore the timeseries features on the SmartBulb type and develop a supervised learning model that given a set of observations on any given day, predicts whether or not the SmartBulb will fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 1. Retrieve Data <a class=\"anchor\" id=\"1.2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The first thing we need to do in order to train our Machine learning Pipeline is to assemble our training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For our binary classification problem, we will use the simple metrics and compound metrics we explored in the previous module as features. The `WillFailNextMonth` compound metric will serve as the label while the `HasEverFailed` metric will serve as a mask to ignore all data after a bulb has failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T17:30:26.317906Z",
     "start_time": "2021-10-29T17:30:26.314766Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List of features that we would like to use in our model\n",
    "features = [\n",
    "            \"MyMetric1\",\n",
    "            \"MyMetric2\",\n",
    "]\n",
    "\n",
    "# We will use this to discard data AFTER a bulb has failed\n",
    "mask = 'MyMask'\n",
    "label = 'MyLabel'\n",
    "columns = features + [mask, label]\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Evaluating Metrics using the methods `.evalMetrics()` is a synchronous operation and should not be called on large datasets. The helper methods we've defined will be getting the results of these evaluated metrics into the memory of our Jupyter notebook. When doing so, we should be conscious of the memory required by our dataset.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We retrieve the features along with the label and mask at a daily interval using our helper function on a subset of our total data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T17:30:31.042965Z",
     "start_time": "2021-10-29T17:30:28.803687Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start = datetime(2016,1,1)\n",
    "end = datetime(2021,1,1)\n",
    "interval = 'DAY'\n",
    "\n",
    "my_spec = c3.EvalMetricsSpec(filter = \"startsWith(id, 'SMBLB1')\",\n",
    "                             expressions = columns,\n",
    "                             start = start,\n",
    "                             end = end,\n",
    "                             interval = interval)\n",
    "\n",
    "df = spec_to_emr_to_df(source_type=c3.SmartBulb,\n",
    "                       spec=my_spec,\n",
    "                       on_the_fly=False,\n",
    "                       overrideMetrics=[None])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 2. Train Test Split <a class=\"anchor\" id=\"1.3\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We have assembled the data we need into a well formatted dataframe and now we can process this dataframe to generate training and testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this example, we will make the simplifying assumption that all SmartBulbs are independent of each other and therefore we can split our timeseries data into train and test set based on the bulbIds. We can do so since this is a completely fabricated classification dataset. It allows us to focus more on the mechanics of the platform as it pertains to model building and deployment without adding the complications of timeseries cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T17:30:31.065667Z",
     "start_time": "2021-10-29T17:30:31.044181Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = df[df[mask] == 0]\n",
    "\n",
    "df_train = df[~df['source'].str.contains(\"SMBLB11|SMBLB12\")]\n",
    "df_test = df[df['source'].str.contains(\"SMBLB11|SMBLB12\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "**Note**: Here we convert our train and test dataframes to c3 `Dataset` type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T17:30:34.782086Z",
     "start_time": "2021-10-29T17:30:34.756533Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train = c3.Dataset.fromPython(df_train[features])\n",
    "y_train = c3.Dataset.fromPython(df_train[label].values)\n",
    "X_test = c3.Dataset.fromPython(df_test[features])\n",
    "y_test = c3.Dataset.fromPython(df_test[label].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Examine the structure of X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T17:30:35.551944Z",
     "start_time": "2021-10-29T17:30:35.512321Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T17:30:50.036640Z",
     "start_time": "2021-10-29T17:30:50.033399Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 3. Specify Preprocessing Pipeline <a class=\"anchor\" id=\"1.4\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this module, we will construct a machine learning pipeline which will consist of a **Preprocessing** step which will perform standardization of features and then maps the data onto its principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We first define the individual `SklearnPipe`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T17:30:56.901200Z",
     "start_time": "2021-10-29T17:30:56.329484Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "help(c3.SklearnPipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Note** that `SklearnPipe` mixes `MLLeafPipe`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First we define the StandardScaler operation. **Note** the structure of the `SklearnTechnique`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T17:30:57.518409Z",
     "start_time": "2021-10-29T17:30:57.512233Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "standardScaler = c3.SklearnPipe(\n",
    "                    name=\"standardScaler\",\n",
    "                    technique=c3.SklearnTechnique(\n",
    "                        # This tells ML pipeline to import sklearn.preprocessing.StandardScaler.\n",
    "                        name=\"preprocessing.StandardScaler\",\n",
    "                        # This tells ML pipeline to call \"transform\" method on sklearn.preprocessing.StandardScaler when we invoke the C3 action process() later.\n",
    "                        processingFunctionName=\"transform\"\n",
    "                    )\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Then we define the PCA operation. **Note** how hyperparameters may be passed to the SklearnTechnique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T17:30:59.154253Z",
     "start_time": "2021-10-29T17:30:59.151621Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pca = c3.SklearnPipe(\n",
    "         name=\"pca\",\n",
    "         technique=c3.SklearnTechnique(\n",
    "             name=\"decomposition.PCA\",\n",
    "             processingFunctionName=\"transform\",\n",
    "             # hyperParameters are passed to sklearn.decomposition.PCA as kwargs\n",
    "#              hyperParameters={\"n_components\": 2}\n",
    "         )\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we can put these pieces together into an `MLSerialPipeline`. **Note** the steps field which consists of an array of `MLSteps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T17:31:00.682575Z",
     "start_time": "2021-10-29T17:31:00.677471Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preprocess_pipeline = c3.MLSerialPipeline(\n",
    "                        name=\"preprocessPipeline\",\n",
    "                        steps=[c3.MLStep(name=\"standardScaler\",\n",
    "                                         pipe=standardScaler),\n",
    "                               c3.MLStep(name=\"pca\",\n",
    "                                         pipe=pca)\n",
    "                              ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 4. Specify Classifier <a class=\"anchor\" id=\"1.5\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For our binary classifier, we will use a LightGBM model which is a gradient boosting framework and especially designed for distributed and efficient computing. The C3 Type `LightGbmPipe` uses the LightGbm framework underneath and its implementation closely follows that of the `SklearnPipe` Type.\n",
    "\n",
    "For more code examples of declaring other MLPipes refer to our documentation: [Code Examples for MLPipe Interface](https://developer.c3.ai/docs/7.24.0/topic/mlpipe-code-examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T17:36:25.151340Z",
     "start_time": "2021-10-29T17:36:25.148047Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "classifier = c3.LightGbmPipe(\n",
    "                    name=\"lgbmClassifier\",\n",
    "                    technique=c3.LightGbmTechnique(\n",
    "                        name=\"LGBMClassifier\",\n",
    "                        processingFunctionName=\"predict\",\n",
    "                        hyperParameters={\n",
    "                                            'boosting_type': 'gbdt',\n",
    "                                            'learning_rate': 0.1,\n",
    "                                            'n_estimators': 2000,\n",
    "                                        }),\n",
    "#                     interpretTechnique=c3.ShapInterpretTechnique(\n",
    "#                                                 name=\"TreeExplainer\"\n",
    "#                                         )\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Note** the interpret technique is used for generating feature contributions. You can investigate this as part of an Advanced Challenge in this module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 5. Create Hetrogeneous Pipeline and Upsert <a class=\"anchor\" id=\"1.6\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now that we have our **preprocessing** and **classification** steps defined, we can combine them into an `MLSerialPipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T17:36:25.805985Z",
     "start_time": "2021-10-29T17:36:25.796576Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "heterogeneousPipeline = c3.MLSerialPipeline(\n",
    "                name=\"myPipeline\",\n",
    "                steps=[c3.MLStep(name=\"preprocess\",\n",
    "                                 pipe=preprocess_pipeline),\n",
    "                       c3.MLStep(name=\"classifier\",\n",
    "                                 pipe=classifier)],\n",
    "                scoringMetrics=c3.MLScoringMetric.toScoringMetricMap(scoringMetricList=[c3.MLAccuracyMetric(), c3.MLPrecisionMetric(), c3.MLF1ScoreMetric()])\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now that he have defined our `MLSerialPipeline`, we can **upsert** it so that it persists on the platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T17:36:26.622656Z",
     "start_time": "2021-10-29T17:36:26.128690Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "upserted_heterogeneousPipeline = heterogeneousPipeline.upsert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now that the `MLSerialPipeline` is upserted on the platform, we can retrieve it via an API call. \n",
    "\n",
    "**Note** that in this example, we're retrieving the machine learning pipeline that we just declared, however now that its persisted on the platform anyone on our team can also retrieve it in order to use in their experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T17:36:26.643435Z",
     "start_time": "2021-10-29T17:36:26.624674Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "retrievedPipeline = upserted_heterogeneousPipeline.get()\n",
    "retrievedPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 6. Train Pipeline <a class=\"anchor\" id=\"1.7\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we can simply train on the platform using the train() method on the `MLSerialPipeline` and passing in our training `Dataset`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Note** - This method of synchronous training should only be performed while experimenting with small datasets. In the upcoming modules we will explore asynchronous training techniques that can be used to train using the distributed framework of the C3 AI suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T17:36:52.292633Z",
     "start_time": "2021-10-29T17:36:34.925702Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "retrievedPipeline = retrievedPipeline.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "retrievedPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T17:36:57.933194Z",
     "start_time": "2021-10-29T17:36:56.863112Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "retrievedPipeline.upsert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 7. Score and Analyze Results <a class=\"anchor\" id=\"1.8\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can score our model on the platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T17:37:10.671682Z",
     "start_time": "2021-10-29T17:37:00.840438Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_score = retrievedPipeline.score(input=X_train, targetOutput=y_train)\n",
    "test_score = retrievedPipeline.score(input=X_test, targetOutput=y_test)\n",
    "print(f'train score = {train_score}')\n",
    "print(f'test score = {test_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can generate predictions on the platform. Note the returned predictions are c3 `Dataset`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T17:37:23.128878Z",
     "start_time": "2021-10-29T17:37:16.266732Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred_train = retrievedPipeline.process(input=X_train)\n",
    "y_pred_test = retrievedPipeline.process(input=X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We convert back to a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T17:37:23.135814Z",
     "start_time": "2021-10-29T17:37:23.130255Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred_train_df = c3.Dataset.toPandas(y_pred_train)\n",
    "y_pred_test_df = c3.Dataset.toPandas(y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we declare a plotting method that will plot the precision and recall curve along with the average precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T17:37:23.512738Z",
     "start_time": "2021-10-29T17:37:23.137181Z"
    },
    "code_folding": [
     4
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "def plot_prec(y_pred_test, y_true_test, y_pred_train, y_true_train, fp=None, save=False):\n",
    "    \n",
    "    prec, rec, thresholds = precision_recall_curve(y_true=y_true_test, probas_pred=y_pred_test)\n",
    "    avg_prec_test = average_precision_score(y_true=y_true_test, y_score=y_pred_test) \n",
    "    lw = 1\n",
    "    plt.figure(figsize=[6, 6])\n",
    "    plt.plot(rec, prec, color='darkorange',\n",
    "         lw=lw, label='Precision-Recall curve(average precision = %0.2f), Testing' % avg_prec_test)\n",
    "    \n",
    "    prec, rec, thresholds = precision_recall_curve(y_true=y_true_train, probas_pred=y_pred_train)\n",
    "    avg_prec_train = average_precision_score(y_true=y_true_train, y_score=y_pred_train) \n",
    "    lw = 1\n",
    "    \n",
    "    plt.plot(rec, prec, color='blue',\n",
    "         lw=lw, label='Precision-Recall curve(average precision = %0.2f), Training' % avg_prec_train)\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    if save:\n",
    "        plt.savefig(fp)\n",
    "    plt.show()\n",
    "    return avg_prec_test, avg_prec_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can use the helper method to plot the precision recall curve for the training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T17:37:24.031324Z",
     "start_time": "2021-10-29T17:37:23.514119Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_avg_prec, train_avg_prec = plot_prec(y_pred_test=y_pred_test_df,\n",
    "                                          y_true_test=df_test[label],\n",
    "                                          y_pred_train=y_pred_train_df,\n",
    "                                          y_true_train=df_train[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T17:37:24.826757Z",
     "start_time": "2021-10-29T17:37:24.823788Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'Average Precision Train: {train_avg_prec}')\n",
    "print(f'Average Precision Test: {test_avg_prec}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Use this section for the Advanced Challenge: Model Interpretability and Feature Contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In order to complete this challenge, go back up to the cell where you specified your classifier and uncomment the lines to include the InterpretTechnique. Re-run your notebook up until this point.\n",
    "\n",
    "You can explore the various explainability frameworks we can use in our documentation here: [ML Pipeline Interpretability](https://developer.c3.ai/docs/7.24.0/topic/pipeline-interpretability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Generate feature contributions using the interpret technique specified on your pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T17:59:02.564082Z",
     "start_time": "2021-10-29T17:58:55.091531Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "interpret_result = retrievedPipeline.interpret(input=X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Extract the feature contributions for each prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T18:10:52.729376Z",
     "start_time": "2021-10-29T18:10:52.558331Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "contributions_dataset = c3.Dataset.fromTensor(\n",
    "    tensor=interpret_result.contributions.subTensor([\":\",\":\",\"1\"]),\n",
    "    axesAsRow=[0],\n",
    "    axesAsColumn=[1, 2])\n",
    "appended = interpret_result.output.extractColumns([\"prediction\"]).appendColumns(dataset=contributions_dataset)\n",
    "contributions_df = c3.Dataset.toPandas(dataset=appended)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T18:10:57.332341Z",
     "start_time": "2021-10-29T18:10:57.323379Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "contributions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure to SYNC your notebook to the server, then CLOSE AND HALT this notebook when you leave.\n",
    "To sync: go to the File menu, Save and Checkpoint your notebook, and then select \"Upload Notebook to C3.ai\", or select the notebook in the tree view (check the box) and hit the \"Sync\" button."
   ]
  }
 ],
 "metadata": {
  "has_local_update": true,
  "is_local": true,
  "is_remote": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "last_sync_time": "2022-04-11T16:42:33.017449"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
